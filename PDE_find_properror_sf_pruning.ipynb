{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PDE_FIND2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computational method to consider\n",
    "comp_str = 'nn' #options are 'nn','finite_differences','splines'\n",
    "\n",
    "#mathematical model\n",
    "model_str = 'fisher' #options are 'diffadv','fisher','fisher_nonlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and format data\n",
    "skip = 20 #number of initial timepoints to skip\n",
    "sample_width = 5 #how much to subsample by (timepoints)\n",
    "normalize = 0 #to normalize data or not during PDE-FIND implementation\n",
    "deg = 2 # degree of polynomial to use in library\n",
    "    \n",
    "#training-validation split\n",
    "trainPerc = .5      # must be between 0 and 1\n",
    "valPerc = 1-trainPerc\n",
    "\n",
    "#number of training-validation splits per data set\n",
    "reals = 1000\n",
    "\n",
    "#how to permute the data\n",
    "shufMethod = 'bins' #options are 'perm' (each point randomly split) , 'noperm' (first \n",
    "                    #trainPerc of timepoints given to training data, rest to validation),\n",
    "                    #'reverse' (last trainperc of timepoints given to training data, rest\n",
    "                    # to validation), 'bins' (grouping local spatiotemporal points randomly)\n",
    "\n",
    "#optimization algorithm\n",
    "algoName = 'Greedy' #options: 'STRidge','Lasso','Greedy'\n",
    "\n",
    "#where to write result\n",
    "write_dir = 'pickle_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data directory, true eqn form, and pruning level for different models\n",
    "if model_str == 'diffadv':\n",
    "    data_dir = \"Data/properror_adasamp_12_7/advection_diffusion_\"\n",
    "    deriv_list = ['u_{xx}','u_{x}']\n",
    "    prune_level = 0.25 \n",
    "    \n",
    "elif model_str == 'fisher':\n",
    "    data_dir = \"Data/fisher/fisher_\"\n",
    "    deriv_list = ['u_{xx}','u','u^2']\n",
    "    prune_level = 0.25\n",
    "    \n",
    "elif model_str == 'fisher_nonlin':\n",
    "    data_dir = \"Data/nonlin_fisher/fisher_nonlin_\"\n",
    "    deriv_list = ['uu_{xx}','u_{x}^2','u','u^2']\n",
    "    prune_level = 0.05\n",
    "    \n",
    "#data files (based on different noise levels) to consider\n",
    "data_files = ['00_' + comp_str,'01_' + comp_str,'05_' + comp_str,'10_' + comp_str,'25_' + comp_str,'50_' + comp_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for d in data_files:\n",
    "\n",
    "    #filename to save at\n",
    "    filename = write_dir + algoName + '_' + d + '_' + shufMethod + '_'+model_str+'_prune_deg_' +str(deg)+ '.npz'\n",
    "    \n",
    "    #list of xi estimates from PDE-FIND with pruning\n",
    "    xi_list = []\n",
    "    #list of xi estimates from PDE-FIND (no pruning)\n",
    "    xi_list_no_prune = []\n",
    "    #list of selected hyperparameters from each simulation\n",
    "    hparams_list = []\n",
    "    #validation score\n",
    "    val_score_list = []\n",
    "    #list of TPR scores for each realization\n",
    "    TP_score_list = []\n",
    "\n",
    "    #load in file\n",
    "    mat = np.load(data_dir + d + '.npy').item()\n",
    "    #create indep. variable grids, ut, theta\n",
    "    t_samp,x_samp,ut,theta,description = diffadv_theta_construct_sf(mat,skip,sample_width,deg)\n",
    "    \n",
    "    #loop through reals\n",
    "    for real in np.arange(reals):\n",
    "    \n",
    "        #split data into train and validation data\n",
    "        # ptrain, pval are indices pertaining to train / validation data : \n",
    "        # i.e., ut[ptrain] = utTrain\n",
    "        utTrain,thetaTrain,ptrain,utVal,thetaVal,pval,utTest,thetaTest,ptest = data_shuf(ut,\n",
    "             theta,shufMethod,trainPerc,valPerc,len(x_samp),len(t_samp),stack=1)\n",
    "\n",
    "        #perform training and validation for given data\n",
    "        xi, hparams, val_score, TP_score = run_PDE_Find_train_val(thetaTrain, utTrain, thetaVal, utVal, algoName,description,deriv_list)\n",
    "                \n",
    "        print \"initial equation is \" + print_pde(xi,description)\n",
    "        print \"initial TPR score is \" + str(TP_TPFPFN(xi,description,deriv_list,0))\n",
    "        \n",
    "        #implement pruning if xi has more than 1 nonzero entry\n",
    "        if len(xi[xi!=0]) > 1:\n",
    "            #perform pruning methodology\n",
    "            xi_new, description_new, thetaTrain_new, thetaVal_new = PDE_FIND_prune_lstsq(xi,utTrain,\n",
    "                                         utVal,thetaTrain,thetaVal,description,val_score,prune_level)\n",
    "            #obtain final validation score\n",
    "            val_score = run_PDE_Find_Test(thetaVal,utVal,xi_new)\n",
    "        else:\n",
    "            xi_new = xi\n",
    "            \n",
    "        print \"updated equation is \" + print_pde(xi_new,description)\n",
    "        print \"Final TP score is \" + str(TP_TPFPFN(xi_new,description,deriv_list,0))\n",
    "        \n",
    "        #add new info to lists\n",
    "        xi_list.append(xi_new)\n",
    "        xi_list_no_prune.append(xi)\n",
    "        hparams_list.append(hparams)\n",
    "        val_score_list.append(val_score)\n",
    "        TP_score_list.append(TP_TPFPFN(xi_new,description,deriv_list,0))\n",
    "           \n",
    "        #save\n",
    "        np.savez(filename,xi_list = xi_list,xi_list_no_prune=xi_list_no_prune,hparams_list=hparams_list,val_score_list=val_score_list,TP_score_list=TP_score_list,\n",
    "                description=description,deriv_list=deriv_list)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
